{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colapse all simulation into a unique .feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_complete_file = 'df_complete_ex3.feather' # https://pandas.pydata.org/docs/user_guide/io.html#feather\n",
    "if ( os.path.exists(df_complete_file) ):\n",
    "    print(f'{df_complete_file} already exists. Reading it.')\n",
    "    df_all = pd.read_feather(df_complete_file)\n",
    "else:\n",
    "    print(f'{df_complete_file} does not exists. Generating it.')\n",
    "    data_files = glob.glob('../output_SA_ex3/SummaryFile_*.feather')\n",
    "    df_all = pd.concat((pd.read_feather(file) for file in data_files), ignore_index=True)\n",
    "    df_all.to_feather(df_complete_file)\n",
    "df_all # 6786560 rows = 2816 samples * 10 repplicates * 241 time points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the QoIs: AUC of each mean population, weithed mean and pooled std of distance from the center in the last time\n",
    "\n",
    "- Invasion: Mean and std of cells' distance from the center - measures the average spread of cells and how spread out (variability) the invasion is.\n",
    "    - Summary over time and replicates: For the last point, take the weighted mean of the means (weights: number of cells) and the pooled std from the stds.\n",
    "- Population: store the population of each cell type over time.\n",
    "    - Summary over time and replicates: Area under curve (AUC) of the average populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary_file = 'df_summary_ex3.feather' # https://pandas.pydata.org/docs/user_guide/io.html#feather\n",
    "if ( os.path.exists(df_summary_file) ):\n",
    "    print(f'{df_summary_file} already exists. Reading it.')\n",
    "    df_summary = pd.read_feather(df_summary_file)\n",
    "else:\n",
    "    print(f'{df_summary_file} does not exists. Generating it.')\n",
    "\n",
    "    # Take the weighted mean and pooled std of replicates in each sample and time\n",
    "    # Define the weighted mean function\n",
    "    def weighted_mean(df, value_col, weight_col):\n",
    "        return (df[value_col] * df[weight_col]).sum(skipna=True) / df[weight_col].sum(skipna=True)\n",
    "    # Define the pooled standard deviation function\n",
    "    def pooled_std(df, std_col, weight_col):\n",
    "        variance = ((df[weight_col]-1) * df[std_col]** 2).sum(skipna=True) / (df[weight_col]-1).sum(skipna=True)\n",
    "        return np.sqrt(variance)\n",
    "\n",
    "    # Apply the weighted mean and pooled std to the desired columns\n",
    "    pop_cols = ['tumor_live', 'tumor_dead', 'm2_live', 'm2_dead']\n",
    "    mean_cols = ['dist_mean_tumor_live', 'dist_mean_tumor_dead', 'dist_mean_m2_live', 'dist_mean_m2_dead']\n",
    "    std_cols = ['dist_std_tumor_live', 'dist_std_tumor_dead', 'dist_std_m2_live', 'dist_std_m2_dead']\n",
    "\n",
    "    weighted_means = df_all.groupby(['sample', 'time']).apply(lambda x: pd.Series({f'weighted_mean_{col_wei}': weighted_mean(x, col_mean, col_wei) for col_mean, col_wei in zip(mean_cols,pop_cols)}))\n",
    "    pooled_stds = df_all.groupby(['sample', 'time']).apply(lambda x: pd.Series({f'pooled_std_{col_wei}': pooled_std(x, col_std, col_wei) for col_std, col_wei in zip(std_cols,pop_cols)}))\n",
    "\n",
    "    df_samples_mean = df_all.groupby(['sample', 'time'], as_index=False).mean()\n",
    "    df_samples_mean = df_samples_mean.merge(weighted_means, on=['sample', 'time'])\n",
    "    df_samples_mean = df_samples_mean.merge(pooled_stds, on=['sample', 'time'])\n",
    "    df_samples_mean = df_samples_mean.drop(columns=['Unnamed: 0','replicate'])\n",
    "    \n",
    "    # Function to calculate AUC using trapezoidal rule for trajectories\n",
    "    def calculate_auc_multiple(group):\n",
    "        auc_values = {}\n",
    "        columns = [col for col in df_samples_mean.columns if col.startswith('weighted_mean_')]\n",
    "        for col in columns: # QoI columns of cell population  \n",
    "            mask_notna = group[col].notna()  \n",
    "            auc_values[f'AUC_{col}'] = np.trapezoid(group[col][mask_notna], group['time'][mask_notna])\n",
    "        return pd.Series(auc_values)\n",
    "    # Group by 'Sample_ID' then calculate AUC for each QOI\n",
    "    df_auc = df_samples_mean.groupby(['sample']).apply(calculate_auc_multiple).reset_index() # calculate the AUC for each QOI\n",
    "    \n",
    "    # Filter the dataframe to last time point\n",
    "    df_summary = df_samples_mean[ df_samples_mean['time'] == 7200.0 ]\n",
    "    # Merge the AUC values of trajectories\n",
    "    df_summary = df_summary.merge(df_auc, on=['sample'])\n",
    "\n",
    "    df_summary.to_feather(df_summary_file)\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(6, 2, figsize=(12, 10))\n",
    "plt.subplots_adjust(hspace=1.8)\n",
    "axes[0, 0].plot(df_summary['sample'], df_summary['AUC_weighted_mean_tumor_live'])\n",
    "axes[0, 0].set_title('AUC - weighted_tumor_live')\n",
    "axes[0, 0].set_xlabel('Sample')\n",
    "axes[0, 1].plot(df_summary['sample'], df_summary['AUC_weighted_mean_tumor_dead'])\n",
    "axes[0, 1].set_title('AUC - weighted_tumor_dead')\n",
    "axes[0, 1].set_xlabel('Sample')\n",
    "\n",
    "axes[1, 0].plot(df_summary['sample'], df_summary['AUC_weighted_mean_m2_live'])\n",
    "axes[1, 0].set_title('AUC - weighted_m2_live')\n",
    "axes[1, 0].set_xlabel('Sample')\n",
    "axes[1, 1].plot(df_summary['sample'], df_summary['AUC_weighted_mean_m2_dead'])\n",
    "axes[1, 1].set_title('AUC - weighted_m2_dead')\n",
    "axes[1, 1].set_xlabel('Sample')\n",
    "\n",
    "axes[2, 0].plot(df_summary['sample'], df_summary['dist_mean_tumor_live'])\n",
    "axes[2, 0].set_title('dist_mean_tumor_live last time point')\n",
    "axes[2, 0].set_xlabel('Sample')\n",
    "axes[2, 1].plot(df_summary['sample'], df_summary['dist_mean_tumor_dead'])\n",
    "axes[2, 1].set_title('dist_mean_tumor_dead last time point')\n",
    "axes[2, 1].set_xlabel('Sample')\n",
    "\n",
    "axes[3, 0].plot(df_summary['sample'], df_summary['dist_mean_m2_live'])\n",
    "axes[3, 0].set_title('dist_mean_m2_live last time point')\n",
    "axes[3, 0].set_xlabel('Sample')\n",
    "axes[3, 1].plot(df_summary['sample'], df_summary['dist_mean_m2_dead'])\n",
    "axes[3, 1].set_title('dist_mean_m2_dead last time point')\n",
    "axes[3, 1].set_xlabel('Sample')\n",
    "\n",
    "axes[4, 0].plot(df_summary['sample'], df_summary['dist_std_tumor_live'])\n",
    "axes[4, 0].set_title('dist_std_tumor_live last time point')\n",
    "axes[4, 0].set_xlabel('Sample')\n",
    "axes[4, 1].plot(df_summary['sample'], df_summary['dist_std_tumor_dead'])\n",
    "axes[4, 1].set_title('dist_std_tumor_dead last time point')\n",
    "axes[4, 1].set_xlabel('Sample')\n",
    "\n",
    "axes[5, 0].plot(df_summary['sample'], df_summary['dist_std_m2_live'])\n",
    "axes[5, 0].set_title('dist_std_m2_live last time point')\n",
    "axes[5, 0].set_xlabel('Sample')\n",
    "axes[5, 1].plot(df_summary['sample'], df_summary['dist_std_m2_dead'])\n",
    "axes[5, 1].set_title('dist_std_m2_dead last time point')\n",
    "axes[5, 1].set_xlabel('Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol\n",
    "from SALib import ProblemSpec\n",
    "from uq_physicell.uq_physicell import PhysiCell_Model, get_rule_index_in_csv\n",
    "import os \n",
    "\n",
    "print(os.getcwd())\n",
    "# Change the directory\n",
    "os.chdir(\"/N/slate/hlimadar/grammar_samples\")\n",
    "\n",
    "PhysiCellModel = PhysiCell_Model(\"Sensitivity_Analysis/ConfigFile.ini\", 'model_tam_egf')\n",
    "# Define parameters of rules in range +/- 20% of the reference value\n",
    "names_parameters = []\n",
    "bounds_parameters = []\n",
    "for key_rule, list_rule in PhysiCellModel.parameters_rules.items():\n",
    "    id_rule = get_rule_index_in_csv(PhysiCellModel.rules, key_rule)\n",
    "    parameter_rule = key_rule.split(',')[-1]\n",
    "    names_parameters.append(list_rule[1])\n",
    "    bounds_parameters.append([float(PhysiCellModel.rules[id_rule][parameter_rule])*0.8, float(PhysiCellModel.rules[id_rule][parameter_rule])*1.2])\n",
    "    # id_rule, name, parametername (saturation, half max, or hill power), value of reference\n",
    "    # print(id_rule, list_rule[1], parameter_rule, PhysiCellModel.rules[id_rule][parameter_rule])\n",
    "\n",
    "# Define SA problem\n",
    "problem = {'names': names_parameters, 'bounds': bounds_parameters}\n",
    "sa_sobol = ProblemSpec(problem)\n",
    "\n",
    "# Sample parameters \n",
    "sa_sobol.sample_sobol(2**6, calc_second_order=True, seed=42) # False: N*(D+2) True: N*(2D+2)\n",
    "\n",
    "# Define a range of sample sizes\n",
    "N = np.array([2**1, 2**2, 2**3, 2**4, 2**5, 2**6])\n",
    "D = sa_sobol.samples.shape[1]\n",
    "sample_sizes = N*(2*D+2) # False: N*(D+2) True: N*(2D+2)\n",
    "\n",
    "dic_analyzes = {}\n",
    "QOI_colums = [col for col in df_summary.columns if ( col.startswith('AUC_') | (col.startswith('dist_')) ) ]\n",
    "print('QOIs: ', QOI_colums)\n",
    "# Analyze the multiple QOIs\n",
    "for col in QOI_colums:\n",
    "    # Set the results\n",
    "    sa_sobol.set_results(df_summary[col].to_numpy())\n",
    "    # Example to plot the first-order indices over sample size\n",
    "    S1_values = []  # Store first-order indices for each sample size\n",
    "    ST_values = []  # Store total-order indices for each sample size\n",
    "    \n",
    "    # Run Sobol analysis for each sample size using subsets of the large sample set\n",
    "    for samp_size in sample_sizes:\n",
    "        # Take the first N samples from the large sample set\n",
    "        param_values_subset = sa_sobol.samples[:samp_size]\n",
    "        Y_subset = sa_sobol.results[:samp_size]\n",
    "        # Perform Sobol analysis on the subset\n",
    "        Si = sobol.analyze(problem, Y_subset, calc_second_order=True)#, print_to_console=True)\n",
    "        # Storage results for comparison\n",
    "        S1_values.append(Si['S1'])\n",
    "        ST_values.append(Si['ST'])\n",
    "    \n",
    "    # Convert to NumPy arrays for plotting\n",
    "    dic_analyzes[col] = {'S1_values': np.array(S1_values), 'ST_values': np.array(ST_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "dic_to_df = {\n",
    "    'QOI': [],\n",
    "    'S1': [],\n",
    "    'ST': [],\n",
    "    'sample_size': [],\n",
    "    'parameter': [],\n",
    "    'rule_label': [],\n",
    "    'rule_parameter': []\n",
    "}\n",
    "\n",
    "dic_rules = {\"cycle_tumor_hfm\": \"Rule 01\", \"cycle_tumor_hp\": \"Rule 01\",\n",
    "             \"necrosis_tumor_hfm\": \"Rule 02\", \"necrosis_tumor_hp\": \"Rule 02\",\n",
    "             \"debris_tumor_sat\": \"Rule 04\", \"debris_tumor_hfm\": \"Rule 04\", \"debris_tumor_hp\": \"Rule 04\",\n",
    "             \"speed_tumor_sat\": \"Rule 06\", \"speed_tumor_hfm\": \"Rule 06\", \"speed_motile_tumor_hp\": \"Rule 06\",\n",
    "             \"persistence_tumor_sat\": \"Rule 07\", \"persistence_tumor_hfm\": \"Rule 07\", \"persistence_tumor_hp\": \"Rule 07\",\n",
    "             \"adhesion_tumor_sat\": \"Rule 08\", \"adhesion_tumor_hfm\": \"Rule 08\", \"adhesion_tumor_hp\": \"Rule 08\",\n",
    "             \"speed_M2_hfm\": \"Rule 22\", \"speed_M2_hp\": \"Rule 22\",\n",
    "             \"debris_M2_sat\": \"Rule 28\", \"debris_M2_hfm\": \"Rule 28\", \"debris_M2_hp\": \"Rule 28\"}\n",
    "\n",
    "for qoi in dic_analyzes.keys():\n",
    "    for par_id in range(dic_analyzes[qoi]['S1_values'].shape[1]):\n",
    "        for sample_id in range(dic_analyzes[qoi]['S1_values'].shape[0]):\n",
    "            dic_to_df['QOI'].append(qoi)\n",
    "            dic_to_df['S1'].append(dic_analyzes[qoi]['S1_values'][sample_id, par_id])\n",
    "            dic_to_df['ST'].append(dic_analyzes[qoi]['ST_values'][sample_id, par_id])\n",
    "            dic_to_df['sample_size'].append(sample_sizes[sample_id])\n",
    "            dic_to_df['parameter'].append(problem['names'][par_id])\n",
    "            dic_to_df['rule_label'].append(dic_rules[problem['names'][par_id]])\n",
    "            dic_to_df['rule_parameter'].append(problem['names'][par_id].split('_')[-1])\n",
    "\n",
    "df_analyzes = pd.DataFrame(dic_to_df)\n",
    "df_analyzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for QOI in dic_analyzes.keys():\n",
    "    if not np.isnan(dic_analyzes[QOI]['S1_values']).all():\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f'QoI: {QOI}')\n",
    "        \n",
    "        # rank of 5 most important parameters\n",
    "        rank_par_idx = np.argsort(dic_analyzes[QOI]['ST_values'][-1, :])[::-1][:5]\n",
    "        top_parameters = np.array(problem[\"names\"])[rank_par_idx]\n",
    "        \n",
    "        # Plotting the indices for each QOI\n",
    "        df_temp = df_analyzes[ df_analyzes['QOI'] == QOI ]\n",
    "        sns.lineplot(df_temp, x='sample_size', y='S1', hue='rule_parameter', style='rule_label', ax=axes[0], markers=True, legend=False)\n",
    "        sns.lineplot(df_temp, x='sample_size', y='ST', hue='rule_parameter', style='rule_label', ax=axes[1], markers=True, legend=False)\n",
    "        \n",
    "        axes[0].set_xlabel('Sample Size')\n",
    "        axes[0].set_ylabel('First-Order Index')\n",
    "        \n",
    "        axes[1].set_xlabel('Sample Size')\n",
    "        axes[1].set_ylabel('Total-Order Index')\n",
    "        \n",
    "        sns.barplot(df_temp[ df_temp['sample_size'] == sample_sizes[-1] ], x='rule_label', y='ST', hue='rule_parameter', ax=axes[2])\n",
    "        axes[2].tick_params(axis='x', labelsize=8)\n",
    "        axes[2].set_xlabel('Parameter')\n",
    "        axes[2].set_ylabel(f'Total-Order Index ({sample_sizes[-1]} samples)')\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'QoI: {QOI} has NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the QoIs: mean and std distance from center and AUC of population for each cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "pop_cols = ['m2_live', 'tumor_live', 'm2_dead', 'tumor_dead']\n",
    "colors = ['green', 'gray' , 'black', 'brown']\n",
    "\n",
    "radius_min = np.array([df_summary['dist_mean_'+pop_cols[0]].min(), df_summary['dist_mean_'+pop_cols[1]].min(), df_summary['dist_mean_'+pop_cols[2]].min(), df_summary['dist_mean_'+pop_cols[3]].min()])\n",
    "radius_mean = np.array([df_summary['dist_mean_'+pop_cols[0]].mean(), df_summary['dist_mean_'+pop_cols[1]].mean(), df_summary['dist_mean_'+pop_cols[2]].mean(), df_summary['dist_mean_'+pop_cols[3]].mean()])\n",
    "radius_max = np.array([df_summary['dist_mean_'+pop_cols[0]].max(), df_summary['dist_mean_'+pop_cols[1]].max(), df_summary['dist_mean_'+pop_cols[2]].max(), df_summary['dist_mean_'+pop_cols[3]].max()])\n",
    "err_radius = np.array([df_summary['dist_std_'+pop_cols[0]].mean(), df_summary['dist_std_'+pop_cols[1]].mean(), df_summary['dist_std_'+pop_cols[2]].mean(),  df_summary['dist_std_'+pop_cols[3]].mean()])\n",
    "\n",
    "theta = np.arange(0,2*np.pi,2*np.pi/len(pop_cols))\n",
    "width = (2*np.pi/len(pop_cols)) + np.pi/18\n",
    "\n",
    "aucs = np.array([df_summary['AUC_weighted_mean_'+pop_cols[0]].mean(), df_summary['AUC_weighted_mean_'+pop_cols[1]].mean(), df_summary['AUC_weighted_mean_'+pop_cols[2]].mean(), df_summary['AUC_weighted_mean_'+pop_cols[3]].mean()])\n",
    "err_aucs = np.array([df_summary['AUC_weighted_mean_'+pop_cols[0]].std(), df_summary['AUC_weighted_mean_'+pop_cols[1]].std(), df_summary['AUC_weighted_mean_'+pop_cols[2]].std(), df_summary['AUC_weighted_mean_'+pop_cols[3]].std()])\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax1 = fig.add_subplot(121, polar=True)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "bars = ax1.bar(theta+np.pi/len(pop_cols), radius_max-radius_min, bottom=radius_min, color=colors, label=pop_cols, width=width, edgecolor='black')\n",
    "bars_mean = ax1.errorbar(theta+np.pi/len(pop_cols), radius_mean, yerr=err_radius, fmt='o', color='black', ecolor='black', elinewidth=2, capsize=10)\n",
    "\n",
    "ax1.set_ylim(0,450)\n",
    "ax1.set_ylabel(r'Distance from center ($\\mu m$)')\n",
    "ax1.set_xticks(theta, labels=[])\n",
    "ax1.spines['polar'].set_visible(False)\n",
    "\n",
    "ax2.bar(pop_cols, aucs, yerr=err_aucs, capsize=10, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('AUC of cell population over time')\n",
    "ax2.yaxis.major.formatter._useMathText = True #(make x10^n)\n",
    "\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initial_condition_random_annulus(fraction=1.0, fileName=None):\n",
    "    # Set a new random seed each time to ensure different random numbers are generated\n",
    "    np.random.seed()\n",
    "    zval = 0;\n",
    "    csv_array = np.empty((0, 4), dtype=[('x', 'float64'), ('y', 'float64'), ('z', 'float64'), ('cell_type', 'U20')])\n",
    "    cell_types = {'tumor': 2000, 'macrophage': 100*fraction, 'CD8 T cell': 100*fraction}\n",
    "    for cell_type in cell_types.keys():\n",
    "        num_cells = cell_types[cell_type]\n",
    "        count_cell = 0\n",
    "        if cell_type == 'tumor':\n",
    "            Radius_inner = 0\n",
    "            Radius_external = 400\n",
    "        else:\n",
    "            Radius_inner = 450\n",
    "            Radius_external = 500\n",
    "        while count_cell < num_cells:\n",
    "            t = 2.0 * np.pi * np.random.uniform() # theta ~ U(0,2pi)\n",
    "            r = np.sqrt(np.random.uniform(Radius_inner**2, Radius_external**2))  # radius ~ sqrt(U(Radius_inner^2, Radius_external^2))\n",
    "            xval = r * np.cos(t)\n",
    "            yval = r * np.sin(t)\n",
    "            # print(xval, yval, zval, cell_type)\n",
    "            csv_array = np.append(csv_array, np.array([(xval, yval, zval, cell_type)], dtype=csv_array.dtype))\n",
    "            count_cell += 1\n",
    "    if fileName:\n",
    "        header=\"x,y,z,type,volume,cycle entry,custom:GFP,custom:sample\"\n",
    "        np.savetxt(fileName, csv_array, header=header, delimiter=',', fmt='%.14f,%.14f,%f,%s', comments='')\n",
    "    else:\n",
    "        return csv_array\n",
    "\n",
    "csv_arrray = initial_condition_random_annulus('initial_condition_random_annulus.csv')\n",
    "csv_arrray = initial_condition_random_annulus()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(csv_arrray, columns=['x', 'y', 'z', 'cell_type'])\n",
    "df = df.astype({'x': 'float', 'y': 'float', 'z': 'float', 'cell_type': 'category'})\n",
    "print(df.info())\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='cell_type', style='cell_type', s=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
