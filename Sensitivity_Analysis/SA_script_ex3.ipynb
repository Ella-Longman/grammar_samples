{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colapse all simulation into a unique .feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_complete_ex3.feather already exists. Reading it.\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ecdf, wasserstein_distance\n",
    "\n",
    "df_complete_file = 'df_complete_ex3.feather' # https://pandas.pydata.org/docs/user_guide/io.html#feather\n",
    "if ( os.path.exists(df_complete_file) ):\n",
    "    print(f'{df_complete_file} already exists. Reading it.')\n",
    "    df_all = pd.read_feather(df_complete_file)\n",
    "else:\n",
    "    print(f'{df_complete_file} does not exists. Generating it.')\n",
    "    data_files = glob.glob('../output_SA_ex3/SummaryFile_*.feather')\n",
    "    df_all = pd.concat((pd.read_feather(file) for file in data_files), ignore_index=True)\n",
    "    # Add the new column with parameter value to IC immune fraction\n",
    "    df_all['IC_immune_frac'] = 1\n",
    "    # Define the values to be assigned to samples 1-8 with diff IC proportions\n",
    "    immune_frac_values = [0.99, 1.01, 0.95, 1.05, 0.9,1.1, 0.8, 1.2]\n",
    "    # Assign the values to the corresponding samples\n",
    "    for i, value in enumerate(immune_frac_values, start=1):\n",
    "        df_all.loc[df_all['sample'] == i, 'IC_immune_frac'] = value\n",
    "\n",
    "    # Save the dataframe to a feather file\n",
    "    df_all.to_feather(df_complete_file)\n",
    "\n",
    "params_list = ['phago_rate_apop', 'phago_rate_nec', 'mac_antinf_secretion_base',\n",
    "       'attack_duration', 'cd8_migr_speed_base', 'damage_apop_sat',\n",
    "       'damage_apop_hfm', 'damage_apop_hp', 'dead_debris_sat',\n",
    "       'dead_debris_hfm', 'dead_debris_hp', 'mac_oxy_proinf_sat',\n",
    "       'mac_oxy_proinf_hfm', 'mac_oxy_proinf_hp', 'mac_oxy_antinf_hfm',\n",
    "       'mac_oxy_antinf_hp', 'cd8_antinf_attack_hfm', 'cd8_antinf_attack_hp',\n",
    "       'cd8_proinf_attack_sat', 'cd8_proinf_attack_hfm',\n",
    "       'cd8_proinf_attack_hp', 'cd8_antinf_speed_hfm', 'cd8_antinf_speed_hp',\n",
    "       'cd8_contact_speed_hfm', 'cd8_contact_speed_hp', 'IC_immune_frac']\n",
    "\n",
    "df_all # 1,264,450 rows = 209 samples * 50 repplicates * 121 time points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the QoIs: AUC of tumor mean population, distribution of cell distance from the center at the last time\n",
    "\n",
    "- Population: store the population of each cell type over time.\n",
    "    - Summary over time and replicates: Area under curve (AUC) of the average populations.\n",
    "- Invasion: Distance of cells from the center - provides insights into the spatial distribution and invasion characteristics of the cells.\n",
    "    - Summary over time and replicates: Pool the distances from replicates and calculate the Wassertein distance between tumor and CD8, and tumor and macrophages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_summary_ex3.feather or df_qoi_ex3.feather does not exists. Generating it.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tumor_apop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/dask_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tumor_apop'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_summary_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_qoi_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exists. Generating it.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Sum tumor_apop and tumor_nec columns into a single column called tumor_dead\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtumor_dead\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtumor_apop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m df_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtumor_nec\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m df_all\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtumor_apop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtumor_nec\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the AUC for each sample and replicate\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dask_env/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/dask_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tumor_apop'"
     ]
    }
   ],
   "source": [
    "df_qoi_file = 'df_qoi_ex3.feather' # https://pandas.pydata.org/docs/user_guide/io.html#feather\n",
    "df_summary_file = 'df_summary_ex3.feather' # https://pandas.pydata.org/docs/user_guide/io.html#feather\n",
    "if ( os.path.exists(df_summary_file) & os.path.exists(df_qoi_file) ):\n",
    "    print(f'{df_summary_file} and {df_qoi_file} already exists. Reading it.')\n",
    "    df_summary = pd.read_feather(df_summary_file)\n",
    "    df_qoi = pd.read_feather(df_qoi_file)\n",
    "else:\n",
    "    print(f'{df_summary_file} or {df_qoi_file} does not exists. Generating it.')\n",
    "    # Sum tumor_apop and tumor_nec columns into a single column called tumor_dead\n",
    "    df_all['tumor_dead'] = df_all['tumor_apop'] + df_all['tumor_nec']\n",
    "    df_all.drop(['tumor_apop', 'tumor_nec'], axis=1, inplace=True)\n",
    "    # Calculate the AUC for each sample and replicate\n",
    "    def calculate_auc_wassertein_dist(group):\n",
    "        auc_values = {}\n",
    "        for col in ['tumor_live', 'tumor_dead','mac_pif_secretion','mac_aif_secretion']: # QoI columns of cell population  \n",
    "            mask_notna = group[col].notna()  \n",
    "            auc_values[f'AUC_{col}'] = np.trapezoid(group[col][mask_notna], group['time'][mask_notna])\n",
    "        wasserstein_values = {}\n",
    "        subgroup = group[group['time'] == 7200.0] # select the last time point\n",
    "        emp_tumor_cdf = ecdf(subgroup['dist_tumor'].values[0])\n",
    "        emp_mac_cdf = ecdf(subgroup['dist_mac'].values[0])\n",
    "        emp_cd8_cdf = ecdf(subgroup['dist_cd8'].values[0])\n",
    "        wasserstein_values['Wasserstein_dist_Mac'] = wasserstein_distance(emp_tumor_cdf.cdf.quantiles, emp_mac_cdf.cdf.quantiles, emp_tumor_cdf.cdf.probabilities, emp_mac_cdf.cdf.probabilities)\n",
    "        wasserstein_values['Wasserstein_dist_CD8'] = wasserstein_distance(emp_tumor_cdf.cdf.quantiles, emp_cd8_cdf.cdf.quantiles, emp_tumor_cdf.cdf.probabilities, emp_cd8_cdf.cdf.probabilities)\n",
    "        return pd.Series({**auc_values, **wasserstein_values})\n",
    "    # Concatenate the dist_tumor_live and dist_tumor_apop and dist_tumor_nec columns into a single column\n",
    "    df_all['dist_tumor'] = df_all.apply(lambda row: np.concatenate([row['dist_tumor_live'], row['dist_tumor_apop'], row['dist_tumor_nec']]), axis=1)\n",
    "    df_all.drop(['dist_tumor_live', 'dist_tumor_apop', 'dist_tumor_nec'], axis=1, inplace=True)\n",
    "    # Group by 'SampleID' and 'replicate' then calculate AUC and Wassertein distance for each QOI - row dimension of samples x replicates\n",
    "    df_qoi = df_all.groupby(['sample', 'replicate']).apply(calculate_auc_wassertein_dist).reset_index()\n",
    "    # Add the parameter values to the QoI dataframe\n",
    "    df_qoi = df_qoi.merge(df_all[['sample', 'replicate'] + params_list], on=['sample', 'replicate'], how='left').drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the QoI for each sample\n",
    "    df_summary = df_qoi.groupby(['sample']).agg({'AUC_tumor_live': ['mean', 'std'], 'AUC_tumor_dead': ['mean', 'std'], 'AUC_mac_pif_secretion': ['mean', 'std'], 'AUC_mac_aif_secretion': ['mean', 'std'], 'Wasserstein_dist_Mac': ['mean', 'std'], 'Wasserstein_dist_CD8': ['mean', 'std']}).reset_index()\n",
    "    df_summary.columns = ['_'.join(col) if col[1] else col[0] for col in df_summary.columns]\n",
    "    # Add the parameter values to the summary dataframe\n",
    "    df_summary = df_summary.merge(df_qoi[['sample'] + params_list], on=['sample'], how='left').drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Save the dataframes to feather files\n",
    "    df_qoi.to_feather(df_qoi_file)\n",
    "    df_summary.to_feather(df_summary_file)\n",
    "\n",
    "print(df_qoi.shape)\n",
    "print(df_summary.shape)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local sensitivity analysis: \n",
    "Give $\\theta^\\star$ as the reference parameter set, we added a multiplicative perturbation $\\delta$, which\n",
    "$$\\theta_i = \\theta^\\star \\times (1\\pm \\delta), \\quad \\delta = \\{0.01, 0.05, 0.1, 0.2\\}$$\n",
    "\n",
    "Absolute sensitivity (A unit increase in $\\theta_i$ changes the output by $S_{\\theta_i}$ units): \n",
    "$$S_{\\theta_i} = \\dfrac{f(\\theta_i)-f(\\theta^\\star)}{\\theta_i - \\theta^\\star}$$\n",
    "Relative sensitivity (Elasticity -  For every $1\\%$ change in $\\theta_i$, the output changes by $\\bar{S}_{\\theta_i} \\times 1\\%$:\n",
    "\n",
    "$$\\bar{S}_{\\theta_i} = \\dfrac{f(\\theta_i)-f(\\theta^\\star)}{\\theta_i - \\theta^\\star}\\cdot \\dfrac{\\theta^\\star}{f(\\theta^\\star)}$$\n",
    "\n",
    "- $S>0:$ As the parameter increases, the output also increases.\n",
    "- $S<0:$ As the parameter increases, the output decreases.\n",
    "- $S=0:$ The output is insensitive to changes in the parameter.\n",
    "- $∣S∣≫1:$ The output is highly sensitive to the parameter; small changes in the parameter will cause large changes in the output.\n",
    "- $∣S∣≪1:$ The output is relatively insensitive to the parameter; large changes in the parameter will have little effect on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ref = 0 # Sample with reference parameters\n",
    "print('Sample with reference parameters: ', df_summary.iloc[sample_ref][params_list].values)\n",
    "\n",
    "# Calculate the sensitivity index\n",
    "QoIs_list = ['AUC_tumor_live_mean', 'AUC_tumor_dead_mean', 'AUC_mac_pif_secretion_mean', 'AUC_mac_aif_secretion_mean', 'Wasserstein_dist_Mac_mean', 'Wasserstein_dist_CD8_mean']\n",
    "# Function to calculate SI using\n",
    "def calculate_SI_multiple():\n",
    "    sensitivity_analysis_dic = {}\n",
    "    # Calculate the delta of parameters\n",
    "    non_zero_values = []; non_zero_params= []\n",
    "    relative_change_params = (df_summary[params_list] - df_summary.iloc[sample_ref][params_list].values) / np.where(df_summary.iloc[sample_ref][params_list].values != 0, df_summary.iloc[sample_ref][params_list].values, 1)\n",
    "    for index, row in relative_change_params.iterrows():\n",
    "        # First index is the reference sample - include the value 1 and skip it\n",
    "        if index == sample_ref: non_zero_values.append(1); non_zero_params.append(params_list[0]); continue\n",
    "        # Get the non-zero values in the row\n",
    "        non_zero_row_values = row[row != 0.0]\n",
    "        # Check if there is exactly one non-zero value\n",
    "        if len(non_zero_row_values) == 1: non_zero_values.append(non_zero_row_values.iloc[0])\n",
    "        else: raise ValueError(f\"Row {index} contains more than one non-zero value or all values are zero. Non zero: {non_zero_row_values} - Row: {row.values}\")\n",
    "        # Parameter name with non-zero value\n",
    "        non_zero_params.append(non_zero_row_values.index[0])\n",
    "    relative_change_params_1d = pd.DataFrame(non_zero_values, columns=['NonZeroValues'])\n",
    "    sensitivity_analysis_dic['SA_parameter'] = np.array(non_zero_params)\n",
    "    sensitivity_analysis_dic['SA_delta'] = relative_change_params_1d.to_numpy()[:,0].round(2)\n",
    "    # Calculate the delta of QoIs\n",
    "    absolute_change_qois = df_summary[QoIs_list] - df_summary.iloc[sample_ref][QoIs_list].values\n",
    "    relative_change_qois = absolute_change_qois / np.where(df_summary.iloc[sample_ref][QoIs_list].values != 0, df_summary.iloc[sample_ref][QoIs_list].values, 1)\n",
    "    absolute_change_params = sensitivity_analysis_dic['SA_delta'] * df_summary.iloc[sample_ref][ sensitivity_analysis_dic['SA_parameter'] ].to_numpy()\n",
    "    for id_qoi, qoi in enumerate(QoIs_list): \n",
    "        sensitivity_analysis_dic[f'SI_{qoi}'] = relative_change_qois.to_numpy()[:,id_qoi]/sensitivity_analysis_dic['SA_delta']\n",
    "        parameter_name = sensitivity_analysis_dic['SA_parameter']\n",
    "        sensitivity_analysis_dic[f'Abs_SI_{qoi}'] = absolute_change_qois.to_numpy()[:,id_qoi]/absolute_change_params\n",
    "    \n",
    "    # Change the delta of reference value to 0 and label it as reference\n",
    "    sensitivity_analysis_dic['SA_delta'][0] = 0\n",
    "    sensitivity_analysis_dic['SA_parameter'][0] = 'Reference'\n",
    "    return pd.DataFrame(sensitivity_analysis_dic)\n",
    "\n",
    "# Group by 'Sample_ID' then calculate AUC for each QOI - row dimension of samples\n",
    "df_SA = calculate_SI_multiple() # calculate the AUC for each QOI\n",
    "df_SA = pd.concat([df_summary['sample'], df_SA], axis=1)\n",
    "\n",
    "# Print the sensitivity index\n",
    "for qoi in QoIs_list:\n",
    "    sample_lowest_qoi = df_summary.iloc[df_summary[qoi].idxmin()]['sample']; sample_highest_qoi = df_summary.iloc[df_summary[qoi].idxmax()]['sample']\n",
    "    sample_lowest_si = df_SA.iloc[df_SA[f'SI_{qoi}'].idxmin()]['sample']; sample_highest_si = df_SA.iloc[df_SA[f'SI_{qoi}'].idxmax()]['sample']\n",
    "    print(f\"QoI: {qoi}\")\n",
    "    print(f\"\\t lowest and highest QoI: {sample_lowest_qoi} ({df_SA.iloc[int(sample_lowest_qoi)]['SA_parameter']}) and {sample_highest_qoi} ({df_SA.iloc[int(sample_highest_qoi)]['SA_parameter']})\")\n",
    "    print(f\"\\t lowest and highest SI: {sample_lowest_si} ({df_SA.iloc[int(sample_lowest_si)]['SA_parameter']}) and {sample_highest_si} ({df_SA.iloc[int(sample_highest_si)]['SA_parameter']})\\n\")\n",
    "df_SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 6))\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "for qoi, ax in zip(QoIs_list, axes.flatten()):\n",
    "    # Plot a box plot for each QoI (x axis: SA_delta, y axis: QoI value)\n",
    "    # ax.boxplot([df_all[df_summary['sample'] == sample][qoi].values for sample in [sample_ref, *samples_deltap1, *samples_deltam1, *samples_deltap5, *samples_deltam5, *samples_deltap10, *samples_deltam10, *samples_deltap20, *samples_deltam20]], showfliers=False)\n",
    "    for delta, color in zip([0.0, 0.01, 0.05, 0.1, 0.2], \n",
    "                            ['gray', 'yellow', 'orange', 'red', 'brown']):\n",
    "        samples = df_SA[ (df_SA['SA_delta'] == delta) | (df_SA['SA_delta'] == -1*delta) ]['sample'].values\n",
    "        # print(delta, samples)\n",
    "        values = df_qoi[df_qoi['sample'].isin(samples)][qoi[:-5]].values\n",
    "        sns.violinplot(x=[delta*100]*len(values), y=values, ax=ax, color=color)\n",
    "    \n",
    "    ax.set_title(qoi[:-5])\n",
    "    ax.set_xlabel('multiplicative perturbation (%)')\n",
    "    ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    ax.yaxis.major.formatter._useMathText = True\n",
    "axes[2, 1].remove()\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the reference sample\n",
    "df_SA = df_SA[df_SA['sample'] != sample_ref]\n",
    "# Convert the multiplicative perturbation to percentage and absolute values\n",
    "df_SA[\"SA_delta\"] = df_SA[\"SA_delta\"].abs() * 100\n",
    "plot = 'Abs_SI' # plot_options =  [\"SI\", \"Abs_SI\", 'Rank']\n",
    "# Plot the parameter rank according to the relatice sensitivity index as heatmap with the parameters vs delta and color as the rank\n",
    "for qoi in QoIs_list:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot == 'SI':\n",
    "        # pivot the dataframe to have the parameter as index, delta as columns and sensitivity index as values\n",
    "        df_SA_pivot_plot = df_SA.pivot(index=\"SA_parameter\", columns=\"SA_delta\", values=\"SI_\"+qoi)\n",
    "    if plot == 'Abs_SI':\n",
    "        # pivot the dataframe to have the parameter as index, delta as columns and absolute sensitivity index as values\n",
    "        df_SA_pivot_plot = df_SA.pivot(index=\"SA_parameter\", columns=\"SA_delta\", values=\"SI_\"+qoi).abs()\n",
    "    if plot == 'Rank':\n",
    "        df_SA_pivot = df_SA.pivot(index=\"SA_parameter\", columns=\"SA_delta\", values=\"SI_\"+qoi).abs()\n",
    "        # rank of each parameter for each multiplicative perturbation ( 1 - highest sensitivity, 2 - second highest sensitivity, etc)\n",
    "        df_SA_pivot_plot = df_SA_pivot.rank(axis=0, method='min', ascending=False)\n",
    "\n",
    "    # print(df_SA_pivot.head(),'\\n')\n",
    "    # print(df_SA_pivot_rank.head(),'\\n')\n",
    "    # \n",
    "    sns.heatmap(df_SA_pivot_plot, cmap='rocket', annot=True,ax=ax)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label('Rank at each multiplicative perturbation')\n",
    "    # draw a line to separate the positive and negative values\n",
    "    ax.axvline(4, color='black', lw=1)\n",
    "    ax.set(title = f'Sensitivity index - {qoi}', xlabel='multiplicative perturbation (%)', ylabel=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Normalized SI in each multiplicative perturbation\n",
    "$$ Normalize SI_{i,j} = \\frac{SI_{i,j}}{\\max SI_{.,j}},$$\n",
    "\n",
    "where $S_{i,j}$ is the sensitivity index in each parameter $i$ and multiplicative pertubation $j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qoi in QoIs_list:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5), gridspec_kw={'width_ratios': [3, 1]})\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    fig.suptitle(f'Normalized sensitivity index for each multiplicative pertubation - {qoi}')\n",
    "    # Round the values to 2 decimal places\n",
    "    # df_SA['SA_delta'] = df_SA['SA_delta'].round(2)\n",
    "    df_SA_pivot = df_SA.pivot(index=\"SA_parameter\", columns=\"SA_delta\", values=\"SI_\"+qoi).abs()\n",
    "    # df of the rank of each parameter for each multiplicative perturbation\n",
    "    # df_SA_pivot_rank = df_SA_pivot.rank(axis=0, method='min', ascending=False)\n",
    "    # convert the rank to rank sensitivity index\n",
    "    # df_SA_pivot_RSI = (df_SA_pivot_rank.shape[0] - df_SA_pivot_rank) / (df_SA_pivot_rank.shape[0] - 1)\n",
    "    df_SA_pivot_RSI = df_SA_pivot / df_SA_pivot.max()\n",
    "    sns.heatmap(df_SA_pivot_RSI, cmap='rocket_r', annot=True,ax=ax1)\n",
    "    cbar = ax1.collections[0].colorbar\n",
    "    cbar.set_label('RSI for each multiplicative perturbation')\n",
    "    ax1.set(xlabel='multiplicative perturbation (%)', ylabel=None)\n",
    "    # draw a line to separate the positive and negative values\n",
    "    ax1.axvline(4, color='black', lw=1)\n",
    "    # Average the RSI across all multiplicative perturbations\n",
    "    df_rsi_mean = df_SA_pivot_RSI.mean(axis=1)\n",
    "    df_rsi_mean = df_rsi_mean.sort_values(ascending=True)\n",
    "    df_rsi_mean.plot(kind='barh', ax=ax2, color='gray')\n",
    "    ax2.set(xlabel='Average RSI', ylabel=None)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of results\n",
    "\n",
    "The local SA does not show clear interpretation, the parameters importance rank changes drastically according the multiplicative perturbation. This coudl be related to the nonlinear behavior: the sensitivity of the output can change depending on the magnitude of the parameters, local vs global: different parameters might dominate at different scales, interactions between the parameters might become more pronounced.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
